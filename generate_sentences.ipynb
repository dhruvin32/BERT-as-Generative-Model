{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x6E6KYM3NvmV"
   },
   "source": [
    "# BERT as conditional generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "--XXZsEs9GOk",
    "outputId": "7dddc70e-77f8-4afa-9b1c-d54ae887558e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.11.15)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.17.5)\n",
      "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.21.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.28.1)\n",
      "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.14.15)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch_pretrained_bert) (2.6.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch_pretrained_bert) (0.15.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.15.0,>=1.14.15->boto3->pytorch_pretrained_bert) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_pretrained_bert #installation of bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "3Yut868c9M-h",
    "outputId": "4ca5169d-0443-49f1-cec0-736f1945a631"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pretained Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ZmwrKOJ9R7-"
   },
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model_version = 'bert-base-uncased'\n",
    "model = BertForMaskedLM.from_pretrained(model_version)\n",
    "model.eval()\n",
    "cuda = torch.cuda.is_available()\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=model_version.endswith(\"uncased\"))\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    return [tokenizer.convert_tokens_to_ids(sent) for sent in batch]\n",
    "\n",
    "def untokenize_batch(batch):\n",
    "    return [tokenizer.convert_ids_to_tokens(sent) for sent in batch]\n",
    "\n",
    "def detokenize(sent):\n",
    "    \"\"\" Roughly detokenizes (mainly undoes wordpiece) \"\"\"\n",
    "    new_sent = []\n",
    "    for i, tok in enumerate(sent):\n",
    "        if tok.startswith(\"##\"):\n",
    "            new_sent[len(new_sent) - 1] = new_sent[len(new_sent) - 1] + tok[2:]\n",
    "        else:\n",
    "            new_sent.append(tok)\n",
    "    return new_sent\n",
    "\n",
    "CLS = '[CLS]'\n",
    "SEP = '[SEP]'\n",
    "MASK = '[MASK]'\n",
    "mask_id = tokenizer.convert_tokens_to_ids([MASK])[0]\n",
    "sep_id = tokenizer.convert_tokens_to_ids([SEP])[0]\n",
    "cls_id = tokenizer.convert_tokens_to_ids([CLS])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic generation step,printer (inbuilt)\n",
    "\n",
    "generating intial masked string with given word at random place (modified given function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M9sn0GD79TeP"
   },
   "outputs": [],
   "source": [
    "def generate_step(out, gen_idx, temperature=None, top_k=0, sample=False, return_list=True):\n",
    "    \"\"\" Generate a word from from out[gen_idx]\n",
    "    \n",
    "    args:\n",
    "        - out (torch.Tensor): tensor of logits of size batch_size x seq_len x vocab_size\n",
    "        - gen_idx (int): location for which to generate for\n",
    "        - top_k (int): if >0, only sample from the top k most probable words\n",
    "        - sample (Bool): if True, sample from full distribution. Overridden by top_k \n",
    "    \"\"\"\n",
    "    logits = out[:, gen_idx]\n",
    "    if temperature is not None:\n",
    "        logits = logits / temperature\n",
    "    if top_k > 0:\n",
    "        kth_vals, kth_idx = logits.topk(top_k, dim=-1)\n",
    "        dist = torch.distributions.categorical.Categorical(logits=kth_vals)\n",
    "        idx = kth_idx.gather(dim=1, index=dist.sample().unsqueeze(-1)).squeeze(-1)\n",
    "    elif sample:\n",
    "        dist = torch.distributions.categorical.Categorical(logits=logits)\n",
    "        idx = dist.sample().squeeze(-1)\n",
    "    else:\n",
    "        idx = torch.argmax(logits, dim=-1)\n",
    "    return idx.tolist() if return_list else idx\n",
    "  \n",
    "  \n",
    "def get_init_text(seed_text,rand_kk, max_len,conditional_word, batch_size = 1, rand_init=False):\n",
    "    \"\"\" Get initial sentence by padding seed_text with either masks or random words to max_len \"\"\"\n",
    "    batch = [seed_text + [MASK] * (max_len) + [SEP] for _ in range(batch_size)]\n",
    "    seed_len = len(seed_text)\n",
    "    for jj in range(batch_size):     #masking our word to generate context revolving around\n",
    "      batch[jj][seed_len + rand_kk] = conditional_word\n",
    "    #if rand_init:\n",
    "    #    for ii in range(max_len):\n",
    "    #        init_idx[seed_len+ii] = np.random.randint(0, len(tokenizer.vocab))\n",
    "    #print(batch)\n",
    "    return tokenize_batch(batch)\n",
    "\n",
    "def printer(sent, should_detokenize=True):\n",
    "    if should_detokenize:\n",
    "        sent = detokenize(sent)[1:-1]\n",
    "    print(\" \".join(sent))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is modified parallel sequential genrator:\n",
    "- genrate initial sequence by calling get_init_text\n",
    "- masked word is only replaced at position where we didn't put our conditonal word\n",
    "- every iteration we make sure at given postion word is replace by given conditional word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Bu0P3Ok9eRL"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "def parallel_sequential_generation(seed_text,conditional_word, batch_size=10, max_len=15, top_k=0, temperature=None, max_iter=300, burnin=200,\n",
    "                                   cuda=False, print_every=10, verbose=True):\n",
    "    \"\"\" Generate for one random position at a timestep\n",
    "    \n",
    "    args:\n",
    "        - burnin: during burn-in period, sample from full distribution; afterwards take argmax\n",
    "    \"\"\"\n",
    "    rand_kk = np.random.randint(0,max_len)\n",
    "    seed_len = len(seed_text)\n",
    "    batch = get_init_text(seed_text,rand_kk, max_len,conditional_word, batch_size)\n",
    "    \n",
    "    for ii in range(max_iter):\n",
    "        kk = np.random.randint(0, max_len)\n",
    "        if(kk != rand_kk):\n",
    "          for jj in range(batch_size):\n",
    "              batch[jj][seed_len+kk] = mask_id\n",
    "          inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)\n",
    "          out = model(inp)\n",
    "          topk = top_k if (ii >= burnin) else 0\n",
    "          idxs = generate_step(out, gen_idx=seed_len+kk, top_k=topk, temperature=temperature, sample=(ii < burnin))\n",
    "          for jj in range(batch_size):\n",
    "              batch[jj][seed_len+kk] = idxs[jj]\n",
    "              \n",
    "          if verbose and np.mod(ii+1, print_every) == 0:\n",
    "              for_print = tokenizer.convert_ids_to_tokens(batch[0])\n",
    "              for_print = for_print[:seed_len+kk+1] + ['(*)'] + for_print[seed_len+kk+1:]\n",
    "              print(\"iter\", ii+1, \" \".join(for_print))\n",
    "            \n",
    "    return untokenize_batch(batch)\n",
    "\n",
    "def generate(n_samples,conditional_word, seed_text=\"[CLS]\", batch_size=10, max_len=25, \n",
    "             generation_mode=\"parallel-sequential\",\n",
    "             sample=True, top_k=100, temperature=1.0, burnin=200, max_iter=500,\n",
    "             cuda=False, print_every=1):\n",
    "    # main generation function to call\n",
    "    sentences = []\n",
    "    n_batches = math.ceil(n_samples / batch_size)\n",
    "    start_time = time.time()\n",
    "    for batch_n in range(n_batches):\n",
    "        if generation_mode == \"parallel-sequential\":\n",
    "            batch = parallel_sequential_generation(seed_text,conditional_word, batch_size=batch_size, max_len=max_len, top_k=top_k,\n",
    "                                                   temperature=temperature, burnin=burnin, max_iter=max_iter, \n",
    "                                                   cuda=cuda, verbose=False)\n",
    "        if (batch_n + 1) % print_every == 0:\n",
    "            print(\"Finished batch %d in %.3fs\" % (batch_n + 1, time.time() - start_time))\n",
    "            start_time = time.time()\n",
    "        \n",
    "        sentences += batch\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating statements of length 10 (you can modify parameters according to requirements)\n",
    "some parameters are:\n",
    "- batch_size : no. of sentences for every conditional word\n",
    "- top_k : selection of word from most likely how much word\n",
    "- max_iter : how many iteration of bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0oAyPVYy9rqw"
   },
   "outputs": [],
   "source": [
    "def generate_statements(synonyms):\n",
    "  n_samples = 1 #5\n",
    "  batch_size = 2 #5\n",
    "  max_len = 10 #40\n",
    "  top_k = 10 #100\n",
    "  temperature = 1.0\n",
    "  generation_mode = \"parallel-sequential\"\n",
    "  leed_out_len = 5 # max_len\n",
    "  burnin = 250\n",
    "  sample = True\n",
    "  max_iter = 500\n",
    "\n",
    "  words = synonyms\n",
    "  #conditional_word = \"happiness\"\n",
    "  sents = []\n",
    "  # Choose the prefix context\n",
    "  for conditional_word in words:\n",
    "    try:\n",
    "      token = tokenizer.convert_tokens_to_ids(conditional_word)[0]\n",
    "      seed_text = \"[CLS]\".split()\n",
    "      bert_sents = generate(n_samples,conditional_word, seed_text=seed_text, batch_size=batch_size, max_len=max_len,\n",
    "                          generation_mode=generation_mode,\n",
    "                          sample=sample, top_k=top_k, temperature=temperature, burnin=burnin, max_iter=max_iter,\n",
    "                          cuda=cuda)\n",
    "      sents.append(bert_sents)\n",
    "    except:\n",
    "      print(\"no token for word\",conditional_word)\n",
    "\n",
    "  return sents\n",
    "\n",
    "  \"\"\"for bert_sents in sents:\n",
    "    for sent in bert_sents:\n",
    "      printer(sent, should_detokenize=True)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to run:\n",
    "- provide input word related to which you want to genrate statements\n",
    "- it will first find synonyms to them.\n",
    "- now for each synonyms it will genrate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "eGCY5T73qvSv",
    "outputId": "c8965cca-1096-4de7-991a-3c5f644e3355"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sadness\n",
      "{'sadness', 'sorrow', 'lugubriousness', 'unhappiness', 'sorrowfulness', 'gloominess'}\n",
      "Finished batch 1 in 8.262s\n",
      "Finished batch 1 in 7.993s\n",
      "no token for word lugubriousness\n",
      "no token for word unhappiness\n",
      "no token for word sorrowfulness\n",
      "no token for word gloominess\n"
     ]
    }
   ],
   "source": [
    "input_word= input()\n",
    "synonyms = [] \n",
    "  \n",
    "for syn in wordnet.synsets(input_word): \n",
    "    for l in syn.lemmas(): \n",
    "        synonyms.append(l.name()) \n",
    "\n",
    "synonyms = list(set(synonyms))\n",
    "print(set(synonyms)) \n",
    "\n",
    "sents = generate_statements(synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing of generated sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "xlyzroRr92AY",
    "outputId": "4e272431-2fc3-40df-fc15-189902e7fa3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated sentences for word sadness\n",
      "\n",
      "1) he now was having a moment of deep sadness .\n",
      "2) baer ' s brown eyes held deep sadness .\n",
      "3) death and sorrow . death and sorrow . sorrow .\n",
      "4) the monk met gray ' s sorrowful look .\n"
     ]
    }
   ],
   "source": [
    "print(\"generated sentences for word\",input_word)\n",
    "print()\n",
    "i=1\n",
    "for bert_sents in sents:\n",
    "  for sent in bert_sents:\n",
    "      print(i,\")\",sep=\"\",end=\" \")\n",
    "      i=i+1\n",
    "      printer(sent, should_detokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x1xWTsF4hB-q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some more output statements:\n",
    "\n",
    "1. generated sentences for good\n",
    "    - \" but are they beneficial ? \" he said .\n",
    "    - \" not exactly mutually beneficial , \" he said .\n",
    "    - the award - winning book gained critical acclaim as well\n",
    "    - it could never work out like this again . well\n",
    "    - but then again , it was only partially effective .\n",
    "    - very strong , very agile , and very effective .\n",
    "\n",
    "\n",
    "2. generated sentences for happy\n",
    "\n",
    "    - well , glad to hear what was going on .\n",
    "    - \" very glad of it , \" she said .\n",
    "    - this was his life . hell , he was happy\n",
    "    - \" put your clothes back on , happy and happy\n",
    "    \n",
    "    \n",
    "3. generated sentences for angry\n",
    "\n",
    "    - chapter 15 : a raging storm . gunfire erupted somewhere - between the good guys and the bad guys .\n",
    "    - he was still a raging monster , but now an image on the wall of monitors across the room .\n",
    "    - meg starred in her first solo television series , with robert altman and pauline kael . born furious\n",
    "    - in 2014 there were performances of the song in copenhagen ( september ) and london ( february 2014 ) furious\n",
    "    - all right , i was kissing him , wild , wild kisses , those kisses , all untold .\n",
    "    - \" anything about that ... ? anything about killing any wild animals in the woods ? \" i asked .\n",
    "    - he heard her coming up for air and mashing her eyes shut . but he was so angry .\n",
    "    - \" hello \" | 1981 hello world | \" hello \" | \" hello world \" | 1979 angry !\n",
    "\n",
    "More example in read me file or you can try giving different input like\n",
    "(fear,horror,envy,beautiful etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assign2_nlp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
